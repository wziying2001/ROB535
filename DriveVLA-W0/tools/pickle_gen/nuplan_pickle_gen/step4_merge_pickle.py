#!/usr/bin/env python3
"""
NuPlan Pickleåˆå¹¶è„šæœ¬ - Step 4
åˆå¹¶æ‰€æœ‰æ•°æ®å¹¶ç”Ÿæˆæœ€ç»ˆpickleï¼ŒåŒ…å«å½’ä¸€åŒ–
"""

import os
import sys
import json
import pickle
import numpy as np
from tqdm import tqdm

# æ·»åŠ è·¯å¾„
sys.path.append("/mnt/vdb1/yingyan.li/repo/OmniSim")
from train.dataset.normalize_pi0 import RunningStats, save

# é…ç½®
INPUT_FILE = "/mnt/vdb1/yingyan.li/repo/OmniSim/dataprocess/intermediate/video_segments.json"
ACTIONS_DIR = "/mnt/vdb1/yingyan.li/repo/OmniSim/dataprocess/intermediate/actions"
COMMANDS_DIR = "/mnt/vdb1/yingyan.li/repo/OmniSim/dataprocess/intermediate/commands"
OUTPUT_DIR = "/mnt/vdb1/yingyan.li/repo/OmniSim/dataprocess/output"
NORMALIZER_DIR = "/mnt/vdb1/yingyan.li/repo/OmniSim/configs/normalizer_nuplan_01"

def load_segment_data(segment_info):
    """åŠ è½½å•ä¸ªæ®µçš„æ‰€æœ‰æ•°æ®"""
    segment_id = segment_info['segment_id']
    
    # åŠ è½½images paths (VQ codes) - ä¿®å¤é”®å
    vq_paths = segment_info.get('npy_paths', segment_info.get('vq_paths', []))
    
    # åŠ è½½actions
    action_file = os.path.join(ACTIONS_DIR, f"{segment_id}.npy")
    if not os.path.exists(action_file):
        return None
    actions = np.load(action_file)  # shape: (frames, 8, 3)
    
    # åŠ è½½commands
    command_file = os.path.join(COMMANDS_DIR, f"{segment_id}.npy")
    if not os.path.exists(command_file):
        return None
    commands = np.load(command_file).tolist()  # list of strings
    
    # éªŒè¯æ•°æ®ä¸€è‡´æ€§
    if len(vq_paths) != len(actions) or len(actions) != len(commands):
        print(f"âŒ æ•°æ®é•¿åº¦ä¸ä¸€è‡´: {segment_id}")
        return None
    
    return {
        "segment_id": segment_id,
        "images": vq_paths,
        "actions": actions,
        "commands": commands
    }

def collect_all_actions(segments_data):
    """æ”¶é›†æ‰€æœ‰actionæ•°æ®ç”¨äºå½’ä¸€åŒ–"""
    all_actions = []
    for segment_data in segments_data:
        actions = segment_data['actions']  # shape: (frames, 8, 3)
        # å±•å¹³ä¸º2D: (frames*8, 3)
        flattened = actions.reshape(-1, 3)
        all_actions.append(flattened)
    
    return np.concatenate(all_actions, axis=0)  # shape: (total_deltas, 3)

def normalize_actions(segments_data, norm_stats):
    """å½’ä¸€åŒ–æ‰€æœ‰actionæ•°æ®"""
    for segment_data in segments_data:
        actions = segment_data['actions'].copy()  # shape: (frames, 8, 3)
        
        # å½’ä¸€åŒ–å…¬å¼: 2 * (x - q01) / (q99 - q01) - 1
        normalized = 2 * (actions - norm_stats.q01) / (norm_stats.q99 - norm_stats.q01 + 1e-8) - 1
        segment_data['actions'] = np.clip(normalized, -1, 1)

def main():
    print("ğŸš€ å¼€å§‹åˆå¹¶Pickleæ•°æ®...")
    
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    os.makedirs(NORMALIZER_DIR, exist_ok=True)
    
    # åŠ è½½è§†é¢‘æ®µä¿¡æ¯
    with open(INPUT_FILE, 'r') as f:
        data = json.load(f)
    
    segments = data['segments']
    print(f"ğŸ“ å¾…å¤„ç†æ®µæ•°: {len(segments)}")
    
    # åŠ è½½æ‰€æœ‰æ®µæ•°æ®
    segments_data = []
    failed_count = 0
    
    for segment_info in tqdm(segments, desc="åŠ è½½æ•°æ®"):
        segment_data = load_segment_data(segment_info)
        if segment_data is not None:
            segments_data.append(segment_data)
        else:
            failed_count += 1
    
    print(f"âœ… æˆåŠŸåŠ è½½: {len(segments_data)} æ®µ")
    print(f"âŒ å¤±è´¥: {failed_count} æ®µ")
    
    if not segments_data:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼")
        return
    
    # è®¡ç®—å½’ä¸€åŒ–ç»Ÿè®¡
    print("ğŸ“Š è®¡ç®—å½’ä¸€åŒ–ç»Ÿè®¡...")
    action_data = collect_all_actions(segments_data)
    print(f"Actionæ•°æ®å½¢çŠ¶: {action_data.shape}")
    
    normalizer = RunningStats()
    normalizer.update(action_data)
    norm_stats = normalizer.get_statistics()
    
    print(f"Mean: {norm_stats.mean}")
    print(f"Std: {norm_stats.std}")
    print(f"Q01: {norm_stats.q01}")
    print(f"Q99: {norm_stats.q99}")
    
    # ä¿å­˜å½’ä¸€åŒ–å‚æ•°
    norm_stats_save = {"libero": norm_stats}  # ä¿æŒä¸ç°æœ‰ç³»ç»Ÿä¸€è‡´
    save(NORMALIZER_DIR, norm_stats_save)
    print(f"ğŸ’¾ å½’ä¸€åŒ–å‚æ•°ä¿å­˜åˆ°: {NORMALIZER_DIR}")
    
    # å½’ä¸€åŒ–actions
    print("ğŸ”„ åº”ç”¨å½’ä¸€åŒ–...")
    normalize_actions(segments_data, norm_stats)
    
    # ç”Ÿæˆæœ€ç»ˆpickleæ ¼å¼
    result_file = []
    for segment_data in segments_data:
        result_file.append({
            "segment_id": segment_data["segment_id"],
            "image": segment_data["images"],
            "action": segment_data["actions"],
            "text": segment_data["commands"]
        })
    
    # ä¿å­˜pickleæ–‡ä»¶
    output_file = os.path.join(OUTPUT_DIR, "nuplan_processed_data_01.pkl")
    with open(output_file, "wb") as f:
        pickle.dump(result_file, f)
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_frames = sum(len(seg["image"]) for seg in result_file)
    avg_frames = total_frames / len(result_file)
    
    print(f"\nğŸ“Š æœ€ç»ˆç»“æœ:")
    print(f"æ€»æ®µæ•°: {len(result_file)}")
    print(f"æ€»å¸§æ•°: {total_frames}")
    print(f"å¹³å‡æ¯æ®µå¸§æ•°: {avg_frames:.1f}")
    
    # Commandåˆ†å¸ƒç»Ÿè®¡
    command_counts = {}
    for segment_data in result_file:
        for cmd in segment_data["text"]:
            command_counts[cmd] = command_counts.get(cmd, 0) + 1
    
    print(f"\nğŸ“Š Commandåˆ†å¸ƒ:")
    for cmd, count in command_counts.items():
        percentage = count / total_frames * 100
        print(f"{cmd}: {count} ({percentage:.1f}%)")
    
    print(f"\nğŸ’¾ æœ€ç»ˆæ–‡ä»¶ä¿å­˜åˆ°: {output_file}")
    print("âœ… Step 4 å®Œæˆï¼")

if __name__ == "__main__":
    main() 