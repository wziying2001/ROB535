import torch
import math
def get_timestep_embedding(
    timesteps: torch.Tensor,
    embedding_dim: int,
    flip_sin_to_cos: bool = False,
    downscale_freq_shift: float = 1,
    scale: float = 1,
    max_period: int = 100,
):
    """
    This matches the implementation in Denoising Diffusion Probabilistic Models: Create sinusoidal timestep embeddings.

    Args
        timesteps (torch.Tensor):
            a 1-D Tensor of N indices, one per batch element. These may be fractional.
        embedding_dim (int):
            the dimension of the output.
        flip_sin_to_cos (bool):
            Whether the embedding order should be `cos, sin` (if True) or `sin, cos` (if False)
        downscale_freq_shift (float):
            Controls the delta between frequencies between dimensions
        scale (float):
            Scaling factor applied to the embeddings.
        max_period (int):
            Controls the maximum frequency of the embeddings
    Returns
        torch.Tensor: an [N x dim] Tensor of positional embeddings.
    """
    assert len(timesteps.shape) == 1, "Timesteps should be a 1d-array"

    half_dim = embedding_dim // 2
    exponent = -math.log(max_period) * torch.arange(
        start=0, end=half_dim, dtype=torch.float32, device=timesteps.device
    )
    exponent = exponent / (half_dim - downscale_freq_shift)

    emb = torch.exp(exponent)
    emb = timesteps[:, None].float() * emb[None, :]

    # scale embeddings
    emb = scale * emb

    # concat sine and cosine embeddings
    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)

    # flip sine and cosine embeddings
    if flip_sin_to_cos:
        emb = torch.cat([emb[:, half_dim:], emb[:, :half_dim]], dim=-1)

    # zero pad
    if embedding_dim % 2 == 1:
        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))
    return emb

def get_fourier_embeds_from_coordinates(embed_dim, coordinates, max_period: int = 100,):
    """
    Args:
        embed_dim: int
        coordinates: a tensor [B x N x C] representing the coordinates of N points in C dimensions
    Returns:
        [B x N x C x embed_dim] tensor of positional embeddings
    """
    half_embed_dim = embed_dim // 2
    B, N, C = coordinates.shape
    emb = max_period ** (torch.arange(half_embed_dim, dtype=torch.float32, device=coordinates.device) / half_embed_dim)
    emb = emb[None, None, None, :] * coordinates[:, :, :, None]
    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)
    return emb

if __name__ == '__main__':
    # timesteps = torch.arange(100)
    # emb = get_timestep_embedding(timesteps, 1280, max_period=10)
    
    # emb1 = get_timestep_embedding(timesteps, 1280, max_period=100000)
    # emb2 = get_timestep_embedding(timesteps, 1280, max_period=100)
    # print('done')

    coordinates = torch.rand((1, 2, 3))
    emb = get_fourier_embeds_from_coordinates(6, coordinates)
    a, b, c = torch.split(emb, dim=2, split_size_or_sections=1)
    print(emb)
    print(emb.shape)
    print(a.shape)